{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preparation\n",
        "This notebook includes the necessary code to:\n",
        "* Import and preprocess the existing tabular dataset\n",
        "* Make http requests to each recall report to pull all raw html data\n",
        "* Process all raw html data into raw text data\n",
        "* Parse text data and extract meaningful phrases to create textual dataset\n",
        "* Save all data and objects necessary to reproduce results again more efficiently\n",
        "* Augment existing data and textual data into dataset for analysis\n",
        "\n"
      ],
      "metadata": {
        "id": "ACdIOynx14jD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "k3rAHMPn_q5Y"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import pickle\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(\"/content/drive\") #mount google drive to load data\n",
        "recalls = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/recalls.csv\") #load data from csv to dataframe\n",
        "#can similarly load data from local csv file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDIXb7cbAWr0",
        "outputId": "d1a8b68e-1d71-4745-ae15-553811983238"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#recalls.columns #columns of raw imported data"
      ],
      "metadata": {
        "id": "k-lSnZCAAcX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert dates to datetime\n",
        "recalls[\"start_date\"] = pd.to_datetime(recalls[\"start_date\"])\n",
        "recalls[\"end_date\"] = pd.to_datetime(recalls[\"end_date\"])"
      ],
      "metadata": {
        "id": "8rha5m6-m3eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#count per year\n",
        "#start_groups = recalls[\"start_date\"].groupby(recalls.start_date.dt.year).value_counts().sum(level=0)\n",
        "\n",
        "#start_groups.plot.line(ylabel=\"count\", xlabel=\"start date year\", rot=\"45\");\n",
        "#[(d, start_groups[d]) for d in start_groups.index]\n",
        "#np.mean(start_groups)"
      ],
      "metadata": {
        "id": "I1DNsE-coaev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CELL ONLY RUN ONCE FIRST TIME \n",
        "\n",
        "#make requests and save data of request reponses for each recall report\n",
        "\n",
        "recall_requests = []\n",
        "\n",
        "#do not use ipv6, lead to faster request time\n",
        "requests.packages.urllib3.util.connection.HAS_IPV6 = False\n",
        "#start session to make many consequtive requests\n",
        "session = requests.Session()\n",
        "\n",
        "#make requests to gather text data from each recall report url\n",
        "for i, url in enumerate(recalls[\"url\"]):\n",
        "  res = session.get(url)\n",
        "  recall_requests.append(res)\n",
        "  \n",
        "  #print(i)\n",
        "\n",
        "#save the list of request response objects as .dat file to load in later\n",
        "#so do not need to wait hours to make requests each time\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/recall_requests.dat\", \"wb\") as f:\n",
        "        pickle.dump(recall_requests, f)"
      ],
      "metadata": {
        "id": "31S1tbeppjzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#USE THIS AFTER REQUESTS ARE MADE\n",
        "\n",
        "#load the saved request response objects\n",
        "loaded_requests = []\n",
        "try:\n",
        "  #can also use local file path\n",
        "  with open(\"/content/drive/MyDrive/Colab Notebooks/recall_requests.dat\", \"rb\") as f:\n",
        "      loaded_requests = pickle.load(f)\n",
        "except:\n",
        "    print(\"unable to load requests data\")"
      ],
      "metadata": {
        "id": "abAxrQ8WvjJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(loaded_requests) #check length of loaded reqeusts is 1338, equal to number of recalls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArqfKqGesh5O",
        "outputId": "f63e61fa-2c4b-4ffe-9732-b72def9a3eee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1338"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CELL ONLY RUN ONCE FIRST TIME \n",
        "\n",
        "#parse the title of each recall to extract text data\n",
        "locations, products, reasons = [], [], []\n",
        "\n",
        "for req in loaded_requests:\n",
        "  soup = bs(req.text, \"html.parser\")\n",
        "  #print(soup.title.text)\n",
        "  #convert to all lower case and split\n",
        "  title_tokens = str(soup.title.text.lower()).split()\n",
        "\n",
        "  #some titles have no useful information\n",
        "  if title_tokens[:3] == [\"recall\", \"notification\", \"report\"]:\n",
        "    locations.append(\"\")\n",
        "    products.append(\"\")\n",
        "    reasons.append(\"\")\n",
        "  else:\n",
        "    location, product, reason = \"\", \"\", \"\"\n",
        "\n",
        "    #get location or state data about firm if available\n",
        "    try:\n",
        "      location = \" \".join(title_tokens[:title_tokens.index(\"firm\")])\n",
        "    except:\n",
        "      pass \n",
        "    locations.append(location)\n",
        "\n",
        "    #get information about the specific product recalled if available\n",
        "    try:\n",
        "      product = \" \".join(title_tokens[title_tokens.index(\"recalls\")+1: title_tokens.index(\"due\")])\n",
        "    except:\n",
        "      try:\n",
        "        product = \" \".join(title_tokens[title_tokens.index(\"recalls\")+1: title_tokens.index(\"that\")])\n",
        "      except:\n",
        "        try:\n",
        "          product = \" \".join(title_tokens[title_tokens.index(\"recalls\")+1: title_tokens.index(\"products\")+1])\n",
        "        except:\n",
        "          try:\n",
        "            product = \" \".join(title_tokens[title_tokens.index(\"for\")+1: title_tokens.index(\"products\")+1])\n",
        "          except:\n",
        "            try:\n",
        "              product = \" \".join(title_tokens[title_tokens.index(\"for\")+1: title_tokens.index(\"due\")])\n",
        "            except:\n",
        "              pass\n",
        "    products.append(product)\n",
        "      \n",
        "    #get specific recall reason data if available\n",
        "    try:\n",
        "      reason = \" \".join(title_tokens[title_tokens.index(\"to\")+1: title_tokens.index(\"|\")])\n",
        "    except:\n",
        "      try:\n",
        "        reason = \" \".join(title_tokens[title_tokens.index(\"that\")+1: title_tokens.index(\"|\")])\n",
        "      except:\n",
        "        try:\n",
        "          reason = \" \".join(title_tokens[title_tokens.index(\"products\")+1: title_tokens.index(\"|\")])\n",
        "        except:\n",
        "          pass\n",
        "    reasons.append(reason)\n",
        "\n",
        "  #print(location,\";\", product, \";\", reason)\n",
        "\n",
        "\n",
        "#save parsed text data, parsing and text data gathering only needs to be run once\n",
        "#save each list of text data as .dat file to be loaded and used later\n",
        "#can also save as .csv to read \n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/locations.dat\", \"wb\") as f:\n",
        "  pickle.dump(locations, f)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/products.dat\", \"wb\") as f:\n",
        "  pickle.dump(products, f)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/reasons.dat\", \"wb\") as f:\n",
        "  pickle.dump(reasons, f)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/locations.csv\", \"w\") as f:\n",
        "  for item in locations:\n",
        "    f.write(item + \",\")\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/products.csv\", \"w\") as f:\n",
        "  for item in products:\n",
        "    f.write(item + \",\")\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/reasons.csv\", \"w\") as f:\n",
        "  for item in reasons:\n",
        "    f.write(item + \",\")"
      ],
      "metadata": {
        "id": "-e3lXhCruFOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/Colab Notebooks/raw_text.txt\", \"w\") as f:\n",
        "  for req in loaded_requests:\n",
        "    f.write(req.text + \"\\n\")\n",
        "#save raw text to txt file"
      ],
      "metadata": {
        "id": "RAu5GIcpV2QJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "locations, products, reasons = [], [], []\n",
        "#load parsed text data from saved files\n",
        "try:\n",
        "  with open(\"/content/drive/MyDrive/Colab Notebooks/locations.dat\", \"rb\") as f:\n",
        "    locations = pickle.load(f)\n",
        "except:\n",
        "  print(\"unable to load locations data\")\n",
        "\n",
        "try:\n",
        "  with open(\"/content/drive/MyDrive/Colab Notebooks/products.dat\", \"rb\") as f:\n",
        "    products = pickle.load(f)\n",
        "except:\n",
        "  print(\"unable to load products data\")\n",
        "\n",
        "try:\n",
        "  with open(\"/content/drive/MyDrive/Colab Notebooks/reasons.dat\", \"rb\") as f:\n",
        "    reasons = pickle.load(f)\n",
        "except:\n",
        "  print(\"unable to load reasons data\")\n",
        "\n",
        "#make sure each one is correct length loaded\n",
        "print(len(locations), len(products), len(reasons))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWzqWePAQWdb",
        "outputId": "a397a6a8-18e3-43ce-90cf-3da7cdf56874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1338 1338 1338\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#augment generated text data with existing data\n",
        "\n",
        "products_df = pd.DataFrame(products)\n",
        "locations_df = pd.DataFrame(locations)\n",
        "reasons_df = pd.DataFrame(reasons)\n",
        "\n",
        "augmented_df = pd.concat([recalls[[\"start_date\", \"end_date\", \"risk_level\", \"quantity_recovered\", \"url\"]], products_df, reasons_df], axis=1)\n",
        "\n",
        "#save as csv to use in analysis\n",
        "augmented_df.to_csv(\"/content/drive/MyDrive/Colab Notebooks/augmented_dataset.csv\")"
      ],
      "metadata": {
        "id": "6ocShYSMOhF1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
